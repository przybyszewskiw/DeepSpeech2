%
% Niniejszy plik stanowi przykład formatowania pracy magisterskiej na
% Wydziale MIM UW.  Szkielet użytych poleceń można wykorzystywać do
% woli, np. formatujac wlasna prace.
%
% Zawartosc merytoryczna stanowi oryginalnosiagniecie
% naukowosciowe Marcina Wolinskiego.  Wszelkie prawa zastrzeżone.
%
% Copyright (c) 2001 by Marcin Woliński <M.Wolinski@gust.org.pl>
% Poprawki spowodowane zmianami przepisów - Marcin Szczuka, 1.10.2004
% Poprawki spowodowane zmianami przepisow i ujednolicenie 
% - Seweryn Karłowicz, 05.05.2006
% Dodanie wielu autorów i tłumaczenia na angielski - Kuba Pochrybniak, 29.11.2016

% dodaj opcję [licencjacka] dla pracy licencjackiej
% dodaj opcję [en] dla wersji angielskiej (mogą być obie: [licencjacka,en])
\documentclass[licencjacka,en]{pracamgr}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{amssymb}
\newcommand{\bibDownloadDate}{\today}


% Dane magistrantów:
\autor{Piotr Ambroszczyk}{385090}
\autori{Łukasz Kondraciuk}{385775}
\autorii{Wojciech Przybyszewski}{386044}
\autoriii{Jan Tabaszewski}{386319}

\title{NVIDIA Deep Speech}
\titlepl{NVIDIA Deep Speech}

%\tytulang{An implementation of a difference blabalizer based on the theory of $\sigma$ -- $\rho$ phetors}

%kierunek: 
% - matematyka, informacyka, ...
% - Mathematics, Computer Science, ...
\kierunek{Computer Science}

% informatyka - nie okreslamy zakresu (opcja zakomentowana)
% matematyka - zakres moze pozostac nieokreslony,
% a jesli ma byc okreslony dla pracy mgr,
% to przyjmuje jedna z wartosci:
% {metod matematycznych w finansach}
% {metod matematycznych w ubezpieczeniach}
% {matematyki stosowanej}
% {nauczania matematyki}
% Dla pracy licencjackiej mamy natomiast
% mozliwosc wpisania takiej wartosci zakresu:
% {Jednoczesnych Studiow Ekonomiczno--Matematycznych}

% \zakres{Tu wpisac, jesli trzeba, jedna z opcji podanych wyzej}

% Praca wykonana pod kierunkiem:
% (podać tytuł/stopień imię i nazwisko opiekuna
% Instytut
% ew. Wydział ew. Uczelnia (jeżeli nie MIM UW))
\opiekun{dr Janina Mincer-Daszkiewicz \\
  Instytut Informatyki\\
}

% miesiąc i~rok:
\date{May 2019}

%Podać dziedzinę wg klasyfikacji Socrates-Erasmus:
\dziedzina{ 
%11.0 Matematyka, Informatyka:\\ 
%11.1 Matematyka\\ 
%11.2 Statystyka\\ 
11.3 Informatyka\\ 
%11.4 Sztuczna inteligencja\\ 
%11.5 Nauki aktuarialne\\
%11.9 Inne nauki matematyczne i informatyczne
}

%Klasyfikacja tematyczna wedlug AMS (matematyka) lub ACM (informatyka)
\klasyfikacja{D. Software\\
  %D.127. Blabalgorithms\\
  %D.127.6. Numerical blabalysis
  }

% Słowa kluczowe:
\keywords{Deep Speech, ASR, Neural Networks, Machine Learning, Python, PyTorch, NVIDIA, RNN, multi-GPU, FP16}

% Tu jest dobre miejsce na Twoje własne makra i~środowiska:
\newtheorem{defi}{Definicja}[section]

% koniec definicji

\begin{document}
\maketitle

%tu idzie streszczenie na strone poczatkowa
\begin{abstract}
  The authors of this thesis focus on implementing scripts for training DeepSpeech2 model for Automatic Speech Recognition. We try to reproduce results obtained by Baidu Research in End-to-End Speech Recognition paper \cite{DS2} using \texttt{PyTorch} framework. We also experiment with obtaining dataset for Polish language and trying DeepSpeech2 model for it. Finally, we provide fully trained models for English and Polish together with statistics about how changing hyperparametrs and architecture impacts model's performance and accuracy.
\end{abstract}

\tableofcontents
%\listoffigures
%\listoftables

\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}
Transcription of spoken language is a crucial problem for many areas of modern technology industry. Being able to communicate with electronic devices not only by touching them, but also by talking to them them is important goal for IT companies. Such devices are more user-friendly so it is for sure beneficial for everybody. To achieve this goal various solutions were proposed and many of then use complex algorithm (e.g. Hidden Markov Models) \cite{DS1}. However it has been shown that the best accuracy can be achieved with Automatic Speech Recognition (ASR) models based on neural networks \cite{DS2}. Our thesis is concentrated on implementing state of the art ASR model DeepSpeech2.

This thesis is realized with the support of NVIDIA company, and its goal is to implement Automatic Speech Recognition (ASR) model DeepSpeech2 described in \cite{DS2}. Authors of DeepSpeech2 prepared it only for recognizing English and Mandarin, so we experiment with applying it to Polish language as well. We think, it is the biggest challenge, since accuracy of the model depends not only on its implementation, but also on the size and diversity of used dataset. Therefore we have to find appropriate one (paying attention to licenses and copyrights) and prepare it adequately. The great size of dataset creates another problem -- we need our model to be able to train on that data in reasonable time and then work in the real time. Last but not least, in order to determine the best hyperparamters we have to run many experiments, collect their results, and finally analyze them.

In order to accomplish our goals we have implemented DeepSpeech2 model using \texttt{PyTorch} deep learning framework, which is supported with CUDA, and is considered to be comfortable to work with. To achieve high performance system we used open-source libraries prepared by NVIDIA which made it possible to train one neural network over multiple GPUs. Another optimization which sped computations up was using half precision floating point numbers (also known as FP16) instead of single precision.  When it came to collecting datasets we found lots of free English utterances with transcriptions. However, for spoken corpus of Polish it was harder as we had to find hundreds of hours of Polish speech collected for university programs and from audiobooks.

Structure of our thesis is following. In Chapter \ref{r:desc} we introduce architecture of DeepSpeech and DeepSpeech2 models in terms of i.a. used layers, data flow and functions. After that in Chapter \ref{r:extens} we present applied optimizations which increased network performance. Next we move on to Chapter \ref{r:hypers} where we describe the results of experiments on model hyperparameters. In Chapter \ref{r:polish} we describe how we modified and trained neural network to detect Polish language and compare obtained results with English model. Finally in Chapter \ref{r:concls} we summarize our experiments, show their results and present final version of the model.    

We divided our work on model for two main parts. The first was preparing appropriate datasets (for English and for Polish) and processing them so they are in format our model can work on -- Łukasz Kondarciuk and Jan Tabaszewski were in charge of this one. The second was implementing model and doing GPU optimisations to it -- this was task for Piotr Ambroszczyk and Wojciech Przybyszewski.
\chapter{Basic model description}\label{r:desc}

DeepSpeech 2 system is a recurrent neural network trained to ingest speech spectrograms and generate text transcriptions.\\\\

\section{Input and Output specification}
Let $\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ...)\}$ be a training set. $x^{(i)}$ is a time-series of variable length where every time-slice is a spectrogram of power
normalized audio clips, so $x^{(i)}_{t,p}$ denotes the power of the $p$’th frequency bin in the audio frame at time $t$. $y^{(i)}$ is a transcription of utterance $x^{(i)}$.\\\\
DS2 network's input is a time-series $x$ and the output is a prediction over characters $p(l_t|x)$ at each output time-step. For english, possible values of $l_t$ are:

\begin{itemize}
  \item letter from 'a' to 'z';
  \item space;
  \item apostrophe;
  \item blank.
\end{itemize}

Adding non-letter characters allows to find word boundaries. Special symbol blank is outputted each time, when the network is unable
to tell, which character is most likely to occur for the current input spectrogram.\\\\

\section{Layers}
The model of the network is composed of one or more convolutional layers, followed by one or more recurrent layers, followed by one or more fully connected layers. Activation function used throughout the network is clipped rectified linear (ReLU) function: 
$$
	\sigma(x) = 
	\begin{cases*}
		0 & for $x < 0$, \\
		20 & for $x > 20$, \\
		x & otherwise.
	\end{cases*}
$$
Recurrent layers appear in a few different variants - standard recurrent layers or Long Short-Term Memory (LSTM) or Gated Recurrent Units (GRU). After the recurrent layers and fully connected layers are applied, we count the output layer $L$ as a softmax of the output of the last layer.\\\\
Softmax function $f : \mathbb{R}^k \rightarrow \mathbb{R}^k$ defined by the formula:
$$
f_i(v) = \frac{e^{v_i}}{\sum_{i=1}^{k} e^{v_j}},
$$
and
$$
f(v) = \big(f_1(v), f_2(v), \ldots, f_k(v) \big).
$$
We basically apply exponential function to each outputed value, and normalize these values, to make sure that probabilities sum up to 1. In our case $k=29$, hence there is 29 possible output characters to distribute probability on (26 letters and 3 special symbols, as described in the previous section).

\section{CTC Loss}
For training of neural network we typically need a function, which would tell us, how good current network's output is. The lesser value this function has, the better results our network achieves. This kind of function is called a \textbf{loss}. Ussually, minimizing value of the loss is a main goal of training. \\\\
In our case, we need a metric, which would tell us, how far probabilities, returned by network, are from desired output: the actual transcription of the track.\\\\
It occurs to be quite a difficult task, since ....\\\\
In \cite{DS3} Graves, A. et al approched this problem by using Connectionist Temporal Classification.
TODO krótkie strzeszczenie, jak to działa. I jak z tego wyliczyć backward propagację. \\\\
More detailed description, how CTC Loss works, can be found in \cite{DS3}.\\\\

\section{Training}
Training is done using the CTC loss function and backpropagation through time algorithm. \\\\
Accuracy of the model is measured by Word Error Rate (WER). TODO opisać co to WER\\\\
TODO Opisać na czym polega trenowanie, co to jest learning rate, co to jest epoka itp \\\\

\section{Generating transcription}
TODO opisać jak działa CTCBeam i przerabianie prawdopodobieństw na słowa. \\\\

For more detail in model architecture please refer to \cite{DS2}.


\chapter{Additional extensions}\label{r:extens}


\chapter{Experiments on architecture and hyperparameters}\label{r:hypers}


\chapter{Recognizing Polish language}\label{r:polish}

\section{Preparing Polish dataset}
\section{Model’s architecture description}
\section{Comparison of model's performance on English and Polish}


\chapter{Conclusions}\label{r:concls}

To sum up, we present \texttt{PyTorch} scripts for training DeepSpeech2 model for ASR. We also present already trained models for English and Polish as well as the results of our experiments justifying using specific hyperparameters and architecture solutions.\\

\begin{thebibliography}{99}
\addcontentsline{toc}{chapter}{Bibliography} 

\bibitem{DS1} Hannun et al. 
\textit{Deep Speech: Scaling up end-to-end speech recognition}, Silicon Valley AI Lab 2014, \href{https://arxiv.org/abs/1412.5567}{https://arxiv.org/abs/1412.5567}
  
\bibitem{DS2} Baidu Research \textit{ Deep Speech 2: End-to-End Speech Recognition in English and Mandarin}, Silicon Valley AI Lab 2015, \href{https://arxiv.org/abs/1512.02595}{https://arxiv.org/abs/1512.02595}

\bibitem{DS3} A. Graves, S. Fernandez, F. Gomez, and J. Schmidhuber. \textit{ Connectionist temporal classification:
Labelling unsegmented sequence data with recurrent neural networks. In ICML, pages 369-376. ACM, 2006. }
 
\end{thebibliography}
All the files were downloaded on \bibDownloadDate

\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% coding: latin-2
%%% End:
