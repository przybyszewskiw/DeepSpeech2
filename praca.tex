%
% Niniejszy plik stanowi przykład formatowania pracy magisterskiej na
% Wydziale MIM UW.  Szkielet użytych poleceń można wykorzystywać do
% woli, np. formatujac wlasna prace.
%
% Zawartosc merytoryczna stanowi oryginalnosiagniecie
% naukowosciowe Marcina Wolinskiego.  Wszelkie prawa zastrzeżone.
%
% Copyright (c) 2001 by Marcin Woliński <M.Wolinski@gust.org.pl>
% Poprawki spowodowane zmianami przepisów - Marcin Szczuka, 1.10.2004
% Poprawki spowodowane zmianami przepisow i ujednolicenie 
% - Seweryn Karłowicz, 05.05.2006
% Dodanie wielu autorów i tłumaczenia na angielski - Kuba Pochrybniak, 29.11.2016

% dodaj opcję [licencjacka] dla pracy licencjackiej
% dodaj opcję [en] dla wersji angielskiej (mogą być obie: [licencjacka,en])
\documentclass[licencjacka,en]{pracamgr}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{amssymb}
\newcommand{\bibDownloadDate}{\today}


% Dane magistrantów:
\autor{Piotr Ambroszczyk}{385090}
\autori{Łukasz Kondraciuk}{385775}
\autorii{Wojciech Przybyszewski}{386044}
\autoriii{Jan Tabaszewski}{386319}

\title{NVIDIA Deep Speech}
\titlepl{NVIDIA Deep Speech}

%\tytulang{An implementation of a difference blabalizer based on the theory of $\sigma$ -- $\rho$ phetors}

%kierunek: 
% - matematyka, informacyka, ...
% - Mathematics, Computer Science, ...
\kierunek{Computer Science}

% informatyka - nie okreslamy zakresu (opcja zakomentowana)
% matematyka - zakres moze pozostac nieokreslony,
% a jesli ma byc okreslony dla pracy mgr,
% to przyjmuje jedna z wartosci:
% {metod matematycznych w finansach}
% {metod matematycznych w ubezpieczeniach}
% {matematyki stosowanej}
% {nauczania matematyki}
% Dla pracy licencjackiej mamy natomiast
% mozliwosc wpisania takiej wartosci zakresu:
% {Jednoczesnych Studiow Ekonomiczno--Matematycznych}

% \zakres{Tu wpisac, jesli trzeba, jedna z opcji podanych wyzej}

% Praca wykonana pod kierunkiem:
% (podać tytuł/stopień imię i nazwisko opiekuna
% Instytut
% ew. Wydział ew. Uczelnia (jeżeli nie MIM UW))
\opiekun{dr Janina Mincer-Daszkiewicz \\
  Instytut Informatyki\\
}

% miesiąc i~rok:
\date{May 2019}

%Podać dziedzinę wg klasyfikacji Socrates-Erasmus:
\dziedzina{ 
%11.0 Matematyka, Informatyka:\\ 
%11.1 Matematyka\\ 
%11.2 Statystyka\\ 
11.3 Informatyka\\ 
%11.4 Sztuczna inteligencja\\ 
%11.5 Nauki aktuarialne\\
%11.9 Inne nauki matematyczne i informatyczne
}

%Klasyfikacja tematyczna wedlug AMS (matematyka) lub ACM (informatyka)
\klasyfikacja{D. Software\\
  %D.127. Blabalgorithms\\
  %D.127.6. Numerical blabalysis
  }

% Słowa kluczowe:
\keywords{Deep Speech, ASR, Neural Networks, Machine Learning, Python, PyTorch, NVIDIA, RNN, multi-GPU, FP16}

% Tu jest dobre miejsce na Twoje własne makra i~środowiska:
\newtheorem{defi}{Definicja}[section]

% koniec definicji

\begin{document}
\maketitle

%tu idzie streszczenie na strone poczatkowa
\begin{abstract}
  The authors of this thesis focus on implementing scripts for training DeepSpeech2 model for Automatic Speech Recognition. We try to reproduce results obtained by Baidu Research in End-to-End Speech Recognition paper \cite{DS2} using \texttt{PyTorch} framework. We also experiment with obtaining dataset for Polish language and trying DeepSpeech2 model for it. Finally, we provide fully trained models for English and Polish together with statistics about how changing hyperparametrs and architecture impacts model's performance and accuracy.
\end{abstract}

\tableofcontents
%\listoffigures
%\listoftables

\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}
Transcription of spoken language is a crucial problem for many areas of modern technology industry. Being able to communicate with electronic devices not only by touching them, but also by talking to them them is an important goal for IT companies. Such devices are more user-friendly so it is for sure beneficial for everybody. To achieve this goal various solutions were proposed and many of then use complex algorithms (e.g. Hidden Markov Models) \cite{DS1}. However it has been shown that the best accuracy can be achieved with Automatic Speech Recognition (ASR) models based on neural networks \cite{DS2}.

Our thesis concentrates on implementing state of the art ASR model DeepSpeech2 described in \cite{DS2} and we realize it with the support of NVIDIA Corporation. Authors of DeepSpeech2 prepared their model only for recognizing English and Mandarin, so we experiment with applying it to Polish language as well. We think, it is the biggest challenge, since accuracy of the model depends not only on its implementation, but also on the size and diversity of used dataset. Therefore we have to find appropriate one (paying attention to licenses and copyrights) and prepare it adequately. Large size of the dataset creates another problem -- we need our model to be able to train on that data in reasonable time and then work in real time. Last but not least, in order to determine the best hyperparamters we have to run many experiments, collect their results, and finally analyze them.

In order to accomplish our goals we have implemented DeepSpeech2 model using \texttt{PyTorch} deep learning framework, which is supported with CUDA, and is considered to be comfortable to work with. To achieve high performance system we used open-source libraries prepared by NVIDIA which made it possible to train one neural network over multiple GPUs. Another optimization which sped computations up was using half precision floating point numbers (also known as FP16) instead of single precision.  When it came to collecting datasets we found lots of free English utterances with transcriptions. However, for spoken corpus of Polish it was harder as we had to find hundreds of hours of Polish speech collected for university programs and from audiobooks.

Structure of our thesis is the following. In Chapter \ref{r:desc} we introduce architecture of DeepSpeech and DeepSpeech2 models in terms of, among others, used layers, data flow and functions. After that in Chapter \ref{r:extens} we present applied optimizations which increased network performance. Next we move on to Chapter \ref{r:hypers} where we describe the results of experiments on model hyperparameters. In Chapter \ref{r:polish} we describe how we modified and trained neural network to detect Polish language and compare obtained results with English model. Finally in Chapter \ref{r:concls} we summarize our experiments, show their results and present final version of the model.

We divided our work on the model into two main parts. The first one consisted of preparing appropriate datasets (for both English and Polish) and processing them to fit the model -- Łukasz Kondraciuk and Jan Tabaszewski were in charge of this part. The second one consisted of implementing the model and applying GPU optimisations to it -- this was the task for Piotr Ambroszczyk and Wojciech Przybyszewski.
\chapter{Basic model description}\label{r:desc}

DeepSpeech 2 system is a recurrent neural network trained to ingest speech spectrograms and generate text transcriptions.

\section{Input and Output specification}
Let $\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ...)\}$ be a training set. $x^{(i)}$ is a time-series of variable length where every time-slice is a spectrogram of power
normalized audio clips, so $x^{(i)}_{t,p}$ denotes the power of the $p$’th frequency bin in the audio frame at time $t$. $y^{(i)}$ is a transcription of utterance $x^{(i)}$.\\\\
DS2 network's input is a time-series $x$ and the output is a prediction over characters $p(l_t|x)$ at each output time-step. For english, possible values of $l_t$ are:

\begin{itemize}
  \item letters from 'a' to 'z';
  \item space;
  \item apostrophe;
  \item blank.
\end{itemize}

Adding non-letter characters allows to find word boundaries. Special symbol blank is outputted each time, when the network is unable
to tell, which character is most likely to occur for the current input spectrogram.

\section{Layers}
The model of the network is composed of one or more convolutional layers, followed by one or more bidirectional recurrent layers \cite{DS4}, followed by one or more fully connected layers. Activation function used throughout the network is clipped rectified linear (ReLU) function:
$$
	\sigma(x) = 
	\begin{cases*}
		0 & for $x < 0$, \\
		20 & for $x > 20$, \\
		x & otherwise.
	\end{cases*}
$$
Recurrent layers appear in a few different variants - standard recurrent layers or Long Short-Term Memory (LSTM) \cite{DS5} or Gated Recurrent Units (GRU) \cite{DS6}. After the recurrent layers and fully connected layers are applied, we count the output layer $L$ as a softmax of the output of the last layer.\\\\
Softmax function $f : \mathbb{R}^k \rightarrow \mathbb{R}^k$ is defined by the formula:
$$
f_i(v) = \frac{e^{v_i}}{\sum_{i=1}^{k} e^{v_j}},
$$
and
$$
f(v) = \big(f_1(v), f_2(v), \ldots, f_k(v) \big).
$$
We basically apply exponential function to each outputed value, and normalize these values, to make sure that probabilities sum up to 1. In our case $k=29$, hence there is 29 possible output characters to distribute probability on (26 letters and 3 special symbols, as described in the previous section).

\section{CTC Loss}
To train a neural network we typically need a function that would tell us how good current network's output is. The lesser value this function has, the better results our model achieves. This kind of function is called a \textbf{loss function}. Usually, minimizing value of the loss function is a main goal of training. \\\\
Loss function used in DS2 is Connectionist Temporal Classification (CTC) \cite{DS3}. To define this loss let's introduce an encoding of a text. Encoding of a given text \textit{S} is done by replacing every character \textit{c} in \textit{S} by any number of characters \textit{c} and blanks "-". Only restriction is that if there are two adjacent identical letters in \textit{S}, they must be separated by a blank "-".\\
For example possible encodings of \textit{"to"} are \textit{"-tttooo"} and \textit{"-tttoo-o"} but only the latter could be an encoding of \textit{"too"}.\\
Now we say that the probability of the actual transcription is the sum of probabilities of all possible encodings of actual transcription that are the same length as the output of the model. The loss simply is the negative logarithm of this probability. Having this we can count derivatives of this loss function with respect to model output and then apply backpropagation through time algorithm to train the network.

\section{Training}
During training test sets are divided into batches of some certain size. Batches are fed one by one into the model and after each one CTC loss is computed and backpropagated and model is updated. The speed in which changes are applied to the model depends on some chosen constant -- learning rate. All test sets make up for an epoch of training and training lasts for many epochs -- until loss doesn't decrease or decreases very slightly.\\\\
Accuracy of the model is measured by Word Error Rate (WER). It is a common metric of the performance of speech recognition systems. Here \cite{DS8} many commercial ASR systems were compared using WER. To measure Word error rate we compare the recognized word sequence with the reference word sequence by transforming the latter to the former and find:

\begin{itemize}
  \item S -- the number of substitutions,
  \item D -- the number of deletions,
  \item I -- the number of insertions,
  \item C -- the number of correct words,
  \item N -- the number of words in the reference (N=S+D+C).
\end{itemize}
Now the WER can be computed as:
$$
WER = \frac{S + D + I}{N} = \frac{S + D + I}{S + D + C}
$$

\section{Generating transcription}
In order to find Word Error Rate of the model one, of course, has to generate some final transcription. It is generated based on both output of the model and a language model (TODO opisać language model). To generate the final transcription we search for transcription \textit{y} that maximizes \textit{Q(y)}, where \textit{Q} is given as follows:
$$
Q(y) = log(\mathbb{P}_{ctc}(y|x)) + \alpha \cdot log(\mathbb{P}_{lm}(y)) + \beta \cdot word\_count(y)
$$
The term $\mathbb{P}_{ctc}(y|x)$ denotes probability of $y$ being a transcrition of utterance $x$ as described in section 1.3 and the term $\mathbb{P}_{lm}(y)$ denotes probability of the sequence $y$ according to the language model. The weight $\alpha$ controls the relative contributions of the language model and the CTC network. The weight $\beta$ encourages more words in the transcription. Both of those parameters are tunable. We use a beam search similar to one described in \cite{DS7} to find the optimal transcription \textit{y}.\\\\
Beam search is an algorithm that iterates over each time-step in the output of DS2 model remembering some constant number (let this number be $BW$ - "Beam Width") of most probable transcriptions up to this point. When considering next time-step, we create $BW \cdot C$ new transcriptions corresponding to appending every possible character to the end of remembered $BW$ transcriptions. After merging identical transcriptions and recalculating their probabilites we again keep $BW$ best ones. In the end we have $BW$ \textit{most probable} transcriptions and just take the best one.\\
In order to efficiently merge transcriptions and recalculate new probabilities for them we must store probabilities $\mathbb{P}_{b}, \mathbb{P}_{nb}, \mathbb{P}_{lm}$ for all kept transcriptions where:
\begin{itemize}
  \item $\mathbb{P}_{b}(y, t)$ = probability of given transcrition y at time-step t but encoding ends with \textit{blank},
  \item $\mathbb{P}_{nb}(y, t)$ = probability of given transcrition y at time-step t but encoding doesn't end with \textit{blank},
  \item $\mathbb{P}_{lm}(y)$ = probability of given transcrition y according to the language model.\\
\end{itemize}
For more detail in model architecture please refer to \cite{DS2}.


\chapter{Additional extensions}\label{r:extens}


\chapter{Experiments on architecture and hyperparameters}\label{r:hypers}


\chapter{Recognizing Polish language}\label{r:polish}

\section{Preparing Polish dataset}
\section{Model’s architecture description}
\section{Comparison of model's performance on English and Polish}


\chapter{Conclusions}\label{r:concls}

To sum up, we present \texttt{PyTorch} scripts for training DeepSpeech2 model for ASR. We also present already trained models for English and Polish as well as the results of our experiments justifying using specific hyperparameters and architecture solutions.\\

\begin{thebibliography}{99}
\addcontentsline{toc}{chapter}{Bibliography} 

\bibitem{DS1} Hannun et al. 
\textit{Deep Speech: Scaling up end-to-end speech recognition}, Silicon Valley AI Lab 2014, \href{https://arxiv.org/abs/1412.5567}{https://arxiv.org/abs/1412.5567}
  
\bibitem{DS2} Baidu Research \textit{Deep Speech 2: End-to-End Speech Recognition in English and Mandarin}, Silicon Valley AI Lab 2015, \href{https://arxiv.org/abs/1512.02595}{https://arxiv.org/abs/1512.02595}

\bibitem{DS3} A. Graves, S. Fernandez, F. Gomez, and J. Schmidhuber. \textit{Connectionist temporal classification:
Labelling unsegmented sequence data with recurrent neural networks. In ICML, pages 369-376. ACM, 2006.}

\bibitem{DS4} M. Schuster and K. K. Paliwal. \textit{Bidirectional recurrent neural networks.} IEEE Transactions on Signal Processing, 45(11):2673–2681, 1997.

\bibitem{DS5} S. Hochreiter and J. Schmidhuber. \textit{Long short--term memory.} Neural Computation, 9(8):1735—1780, 1997.

\bibitem{DS6} K. Cho, B. Van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio. \textit{Learning phrase representations using rnn encoder-decoder for statistical machine translation.} In EMNLP, 2014. \href{https://arxiv.org/pdf/1406.1078.pdf}{https://arxiv.org/pdf/1406.1078.pdf}

\bibitem{DS7} A. Y. Hannun, A. L. Maas, D. Jurafsky, and A. Y. Ng. \textit{First-pass large vocabulary continuous speech recognition using bi-directional recurrent DNNs.} abs/1408.2873, 2014. \href{http://arxiv.org/abs/1408.2873}{http://arxiv.org/abs/1408.2873}

\bibitem{DS8} Bohouta, Gamal \& Këpuska, Veton. (2017). \textit{Comparing Speech Recognition Systems (Microsoft API, Google API And CMU Sphinx).} Int. Journal of Engineering Research and Application. 2248-9622. 20-24. 10.9790/9622-0703022024.

\end{thebibliography}
All the files were downloaded on \bibDownloadDate

\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% coding: latin-2
%%% End:
